{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLRUp6qmM03QxYsGAi6PU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaifkh20/nn_z_to_h/blob/main/makemore_a_z.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Syf2D7XRHq-m",
        "outputId": "d9887037-a5f6-4a9a-c2ff-42c51c4a8f24"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8a7e4b8-5c53-4e55-9ae8-a67ef1231c5d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e8a7e4b8-5c53-4e55-9ae8-a67ef1231c5d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving names(1).txt to names(1).txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names(1).txt').read().splitlines()"
      ],
      "metadata": {
        "id": "sC6WMvBoLqyS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "rW3HrpsZcQs_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.zeros((27,27),dtype=torch.int32)"
      ],
      "metadata": {
        "id": "iXnMpq_KcjpC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "bywaq8sacp7L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1,ch2 in zip(chs,chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "\n",
        "    N[ix1,ix2] +=1\n"
      ],
      "metadata": {
        "id": "gJhXt9YUdrSY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N[0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIJInnSd9Wb",
        "outputId": "1b08da49-1762-405f-a941-ead48509743c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
              "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
              "         134,  535,  929], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "p = N[ix].float()\n",
        "    p/=p.sum()\n",
        "this is inneficient\n",
        "\n",
        "what is efficient is :\n",
        "P/=P.sum(1,keepdim=true)  "
      ],
      "metadata": {
        "id": "y4htDO3ixylf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model smoothing instead of assigning not encountered pair of words as 0 we make it 1"
      ],
      "metadata": {
        "id": "CSOFP1kgUu3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = (N+1).float()\n",
        "P/=P.sum(1,keepdim=True)"
      ],
      "metadata": {
        "id": "ssc6CmNMyBMF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial function is draw samples from probability distribution"
      ],
      "metadata": {
        "id": "mYiP4GOgXQsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "for i in range(5):\n",
        "  ix = 0\n",
        "  out = []\n",
        "  while True:\n",
        "    p = P[ix]\n",
        "    ix = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix==0:\n",
        "      break\n",
        "\n",
        "\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "id": "gfQvziyGd_Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8d1f8d-de5b-4e74-98a4-044d4f2596ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junide.\n",
            "janasah.\n",
            "p.\n",
            "cony.\n",
            "a.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch\n",
        "\n",
        "P = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "# Sum across rows (axis 0) without keepdim\n",
        "row_sums = P.sum(0)  # Output: tensor([5, 7, 9])\n",
        "\n",
        "# Sum across columns (axis 1) without keepdim\n",
        "column_sums = P.sum(1)  # Output: tensor([ 6, 15])\n",
        "\n",
        "# Sum across rows (axis 0)\n",
        "row_sums = P.sum(0, keepdim=True)  # Output: tensor([[5, 7, 9]])\n",
        "\n",
        "# Sum across columns (axis 1)\n",
        "column_sums = P.sum(1, keepdim=True)  # Output: tensor([[ 6],\n",
        "                                             #          [15]])"
      ],
      "metadata": {
        "id": "VNfzuTE1u6bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood=0.0\n",
        "n=0\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1,ch2 in zip(chs,chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    prob = P[ix1,ix2]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    n+=1\n",
        "\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reVpEHVKiItS",
        "outputId": "828a7c09-0819-4faa-e7e8-63cf6e608fce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nll=tensor(559951.5625)\n",
            "nll/n=tensor(2.4544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training set of bigram (x,y)"
      ],
      "metadata": {
        "id": "Ch0C6M7b2GOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs,ys = [],[]\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1,ch2 in zip(chs,chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys) #the correct y\n",
        "\n",
        "xs.shape,xs"
      ],
      "metadata": {
        "id": "bpE3iMFlTgyV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef125b44-502c-4ca1-b1c7-2e550387d60d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5]), tensor([ 0,  5, 13, 13,  1]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs,num_classes=27).float()\n",
        "xenc,xenc.shape"
      ],
      "metadata": {
        "id": "kk_33x1Q2tQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 (Input Layer) -> 27 neuron (Second Layer)"
      ],
      "metadata": {
        "id": "B5v4ZPUC9PaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly initialize 27 Weights for each neurons(27)\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn([27,27],generator=g,requires_grad = True)"
      ],
      "metadata": {
        "id": "YXYmcvgPNFhT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(100):\n",
        "  xenc = F.one_hot(xs,num_classes=27).float() #input to nnet\n",
        "  logits = xenc @ W #predict log-counts\n",
        "  counts = logits.exp() #counts equivalent to N\n",
        "  prob = counts/counts.sum(1,keepdims=True) #prob for next char (softmax)\n",
        "  loss = -prob[torch.arange(5),ys].log().mean() + 1*(W**2).mean() #negative log-likelihood + regularization term\n",
        "\n",
        "  print(loss.item())\n",
        "\n",
        "  W.grad = None #set it to zero\n",
        "  loss.backward()\n",
        "\n",
        "  W.data += -0.1 * W.grad #update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilXcf95Y8PH4",
        "outputId": "aa090b76-1f68-490b-d286-81dd4c581515"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.035191774368286\n",
            "2.02927827835083\n",
            "2.023423671722412\n",
            "2.017627000808716\n",
            "2.011887788772583\n",
            "2.0062057971954346\n",
            "2.000580072402954\n",
            "1.9950106143951416\n",
            "1.9894965887069702\n",
            "1.984037160873413\n",
            "1.9786324501037598\n",
            "1.9732813835144043\n",
            "1.9679837226867676\n",
            "1.9627389907836914\n",
            "1.9575464725494385\n",
            "1.9524058103561401\n",
            "1.9473164081573486\n",
            "1.9422777891159058\n",
            "1.9372892379760742\n",
            "1.9323503971099854\n",
            "1.927460789680481\n",
            "1.9226198196411133\n",
            "1.9178273677825928\n",
            "1.913082242012024\n",
            "1.9083845615386963\n",
            "1.903733253479004\n",
            "1.8991281986236572\n",
            "1.8945691585540771\n",
            "1.8900549411773682\n",
            "1.8855857849121094\n",
            "1.8811602592468262\n",
            "1.8767790794372559\n",
            "1.8724408149719238\n",
            "1.8681455850601196\n",
            "1.8638924360275269\n",
            "1.8596813678741455\n",
            "1.8555113077163696\n",
            "1.8513824939727783\n",
            "1.8472940921783447\n",
            "1.843245506286621\n",
            "1.8392366170883179\n",
            "1.8352668285369873\n",
            "1.8313356637954712\n",
            "1.8274425268173218\n",
            "1.823587417602539\n",
            "1.8197696208953857\n",
            "1.815988540649414\n",
            "1.8122440576553345\n",
            "1.8085355758666992\n",
            "1.8048629760742188\n",
            "1.801225185394287\n",
            "1.7976226806640625\n",
            "1.7940540313720703\n",
            "1.7905198335647583\n",
            "1.7870192527770996\n",
            "1.783551573753357\n",
            "1.7801170349121094\n",
            "1.7767146825790405\n",
            "1.7733445167541504\n",
            "1.7700058221817017\n",
            "1.7666985988616943\n",
            "1.7634222507476807\n",
            "1.760176420211792\n",
            "1.7569608688354492\n",
            "1.7537751197814941\n",
            "1.750618815422058\n",
            "1.7474915981292725\n",
            "1.744393229484558\n",
            "1.7413232326507568\n",
            "1.738281488418579\n",
            "1.7352674007415771\n",
            "1.7322807312011719\n",
            "1.729321002960205\n",
            "1.7263882160186768\n",
            "1.7234818935394287\n",
            "1.7206015586853027\n",
            "1.7177469730377197\n",
            "1.7149181365966797\n",
            "1.7121140956878662\n",
            "1.7093353271484375\n",
            "1.7065811157226562\n",
            "1.7038512229919434\n",
            "1.7011451721191406\n",
            "1.698462724685669\n",
            "1.6958039999008179\n",
            "1.6931681632995605\n",
            "1.690555453300476\n",
            "1.6879651546478271\n",
            "1.6853973865509033\n",
            "1.6828515529632568\n",
            "1.6803271770477295\n",
            "1.6778247356414795\n",
            "1.6753432750701904\n",
            "1.6728830337524414\n",
            "1.6704434156417847\n",
            "1.6680244207382202\n",
            "1.6656255722045898\n",
            "1.6632468700408936\n",
            "1.6608879566192627\n",
            "1.658548355102539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob"
      ],
      "metadata": {
        "id": "XY48tCi68gPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f21e48-71b2-4c27-cac3-c2b25f741bee"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0271, 0.0059, 0.0072, 0.0026, 0.0095, 0.6126, 0.0017, 0.0126, 0.0079,\n",
              "         0.0162, 0.0048, 0.0147, 0.0055, 0.0050, 0.0235, 0.0591, 0.0270, 0.0016,\n",
              "         0.0134, 0.0034, 0.0173, 0.0064, 0.0018, 0.0110, 0.0069, 0.0483, 0.0470],\n",
              "        [0.0129, 0.0282, 0.0113, 0.0207, 0.0483, 0.0128, 0.0047, 0.0145, 0.0048,\n",
              "         0.0133, 0.0258, 0.0105, 0.0056, 0.6310, 0.0054, 0.0138, 0.0129, 0.0024,\n",
              "         0.0310, 0.0099, 0.0196, 0.0132, 0.0201, 0.0014, 0.0058, 0.0012, 0.0192],\n",
              "        [0.0101, 0.3875, 0.0139, 0.0106, 0.0172, 0.0071, 0.0088, 0.0084, 0.0239,\n",
              "         0.0060, 0.0030, 0.0189, 0.0049, 0.3660, 0.0054, 0.0043, 0.0157, 0.0046,\n",
              "         0.0168, 0.0024, 0.0073, 0.0031, 0.0050, 0.0072, 0.0237, 0.0136, 0.0048],\n",
              "        [0.0101, 0.3875, 0.0139, 0.0106, 0.0172, 0.0071, 0.0088, 0.0084, 0.0239,\n",
              "         0.0060, 0.0030, 0.0189, 0.0049, 0.3660, 0.0054, 0.0043, 0.0157, 0.0046,\n",
              "         0.0168, 0.0024, 0.0073, 0.0031, 0.0050, 0.0072, 0.0237, 0.0136, 0.0048],\n",
              "        [0.6119, 0.0045, 0.0171, 0.0051, 0.0237, 0.0139, 0.0353, 0.0066, 0.0063,\n",
              "         0.0026, 0.0340, 0.0044, 0.0333, 0.0056, 0.0109, 0.0098, 0.0175, 0.0040,\n",
              "         0.0313, 0.0215, 0.0193, 0.0139, 0.0027, 0.0146, 0.0251, 0.0206, 0.0047]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf_ZnPX3UNR0"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}